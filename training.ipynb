{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"large_synthetic_email_sentiment_dataset.csv\")\n",
    "X = df[\"email\"]\n",
    "y = df[[\"politeness_formality\", \"emotional_tone\", \"clarity_constructiveness\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Split temp into validation and test\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertMultiOutputRegressor(nn.Module):\n",
    "    def __init__(self, model_name='bert-base-uncased', num_outputs=3):\n",
    "        super(BertMultiOutputRegressor, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        self.regressor = nn.Linear(self.bert.config.hidden_size, num_outputs)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        dropped_out = self.dropout(pooled_output)\n",
    "        print(\"Called\")\n",
    "        return self.regressor(dropped_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kparv\\Downloads\\49595\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "class EmailDataset(Dataset):\n",
    "    def __init__(self, texts, targets, tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts.iloc[idx])\n",
    "        target = self.targets.iloc[idx].values.astype(float)\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'targets': torch.tensor(target, dtype=torch.float)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = EmailDataset(X_train, y_train, tokenizer)\n",
    "val_dataset = EmailDataset(X_val, y_val, tokenizer)\n",
    "test_dataset = EmailDataset(X_test, y_test, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertMultiOutputRegressor()\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
    "loss_fn = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs shape  tensor([[0.7124, 0.4564, 0.7135],\n",
      "        [0.7715, 0.6694, 0.7322],\n",
      "        [0.4514, 0.1939, 0.4274],\n",
      "        [0.8706, 0.8411, 0.8100],\n",
      "        [0.8809, 0.7377, 0.7407],\n",
      "        [0.8022, 0.4968, 0.7711],\n",
      "        [0.8213, 0.4761, 0.7679],\n",
      "        [0.4811, 0.2148, 0.4349],\n",
      "        [0.7956, 0.4727, 0.7838],\n",
      "        [1.0149, 0.9235, 0.8360],\n",
      "        [0.7201, 0.4946, 0.7463],\n",
      "        [0.8694, 0.5551, 0.7635],\n",
      "        [0.6448, 0.3388, 0.5216],\n",
      "        [0.2877, 0.2686, 0.4997],\n",
      "        [0.3012, 0.1473, 0.3550],\n",
      "        [0.4442, 0.0998, 0.2923]], grad_fn=<AddmmBackward0>)\n",
      "train loss  0.027096854522824287\n",
      "outputs shape  tensor([[0.5466, 0.2968, 0.4237],\n",
      "        [0.8334, 0.8366, 0.7042],\n",
      "        [0.3107, 0.1818, 0.2840],\n",
      "        [0.8751, 0.7484, 0.9817],\n",
      "        [0.5655, 0.3866, 0.4032],\n",
      "        [0.5116, 0.4591, 0.5017],\n",
      "        [0.8828, 0.4244, 0.7548],\n",
      "        [0.3815, 0.2625, 0.3957],\n",
      "        [0.7364, 0.4257, 0.7288],\n",
      "        [0.5092, 0.3109, 0.4840],\n",
      "        [0.3031, 0.0959, 0.3623],\n",
      "        [0.8820, 0.6144, 0.7935],\n",
      "        [0.8428, 0.6308, 0.7221],\n",
      "        [0.8664, 0.5148, 0.6637],\n",
      "        [0.7110, 0.5774, 0.8770],\n",
      "        [0.8379, 0.5823, 0.6092]], grad_fn=<AddmmBackward0>)\n",
      "train loss  0.028045428916811943\n",
      "outputs shape  tensor([[0.5309, 0.3406, 0.5044],\n",
      "        [0.8055, 0.7869, 0.7203],\n",
      "        [0.5890, 0.3668, 0.4921],\n",
      "        [0.8663, 0.6752, 0.7811],\n",
      "        [0.8990, 0.9309, 0.7145],\n",
      "        [0.8105, 0.5072, 0.6119],\n",
      "        [0.9130, 0.4602, 0.6392],\n",
      "        [0.8864, 0.6807, 0.7610],\n",
      "        [0.8613, 0.4892, 0.7737],\n",
      "        [0.8119, 0.6279, 0.7993],\n",
      "        [0.3872, 0.1788, 0.3310],\n",
      "        [0.8267, 0.9289, 0.7496],\n",
      "        [0.7719, 0.5353, 0.9338],\n",
      "        [0.3662, 0.1545, 0.3983],\n",
      "        [0.5852, 0.4186, 0.5158],\n",
      "        [0.5822, 0.4167, 0.4627]], grad_fn=<AddmmBackward0>)\n",
      "train loss  0.022975103929638863\n",
      "outputs shape  tensor([[0.4319, 0.2681, 0.4508],\n",
      "        [0.8376, 0.4866, 0.5386],\n",
      "        [0.7557, 0.5754, 0.5472],\n",
      "        [0.8803, 0.9483, 0.8991],\n",
      "        [0.6872, 0.3583, 0.6217],\n",
      "        [0.6657, 0.5509, 0.5746],\n",
      "        [0.3255, 0.1244, 0.2871],\n",
      "        [0.9130, 0.3680, 0.6831],\n",
      "        [0.7561, 0.5343, 0.5434],\n",
      "        [0.4620, 0.2627, 0.4613],\n",
      "        [0.4379, 0.2520, 0.4331],\n",
      "        [0.8375, 0.4974, 0.7407],\n",
      "        [0.7984, 0.4088, 0.7379],\n",
      "        [0.8405, 0.6547, 0.8828],\n",
      "        [0.8805, 0.5475, 0.6129],\n",
      "        [0.9258, 0.8673, 0.7572]], grad_fn=<AddmmBackward0>)\n",
      "train loss  0.024236788973212242\n",
      "outputs shape  tensor([[0.8281, 0.5538, 0.5675],\n",
      "        [0.8934, 0.5189, 0.7707],\n",
      "        [0.9900, 0.9161, 0.8267],\n",
      "        [0.8190, 0.4618, 0.7566],\n",
      "        [0.5354, 0.3458, 0.3891],\n",
      "        [0.8789, 0.8567, 0.7777],\n",
      "        [0.7389, 0.5530, 0.6741],\n",
      "        [0.7400, 0.5729, 0.5692],\n",
      "        [0.5394, 0.3914, 0.4527],\n",
      "        [0.7744, 0.4743, 0.6042],\n",
      "        [0.7140, 0.4276, 0.6756],\n",
      "        [0.4662, 0.3254, 0.4272],\n",
      "        [0.3641, 0.1044, 0.3380],\n",
      "        [0.8126, 0.8104, 0.8289],\n",
      "        [0.8236, 0.4491, 0.6541],\n",
      "        [0.3937, 0.1822, 0.4508]], grad_fn=<AddmmBackward0>)\n",
      "train loss  0.02501504123210907\n",
      "outputs shape  tensor([[0.9783, 0.6415, 0.9496],\n",
      "        [0.8751, 0.5768, 0.8375],\n",
      "        [0.7932, 0.8381, 0.7703],\n",
      "        [0.8642, 0.5721, 0.7336],\n",
      "        [0.8648, 0.5826, 0.7817],\n",
      "        [0.9290, 0.4380, 0.7846],\n",
      "        [0.5986, 0.4017, 0.5370],\n",
      "        [0.5427, 0.3413, 0.5054],\n",
      "        [0.7333, 0.5382, 0.7422],\n",
      "        [0.8224, 0.3764, 0.7165],\n",
      "        [0.3651, 0.1430, 0.1830],\n",
      "        [0.7566, 0.5118, 0.5844],\n",
      "        [0.5683, 0.2988, 0.5176],\n",
      "        [0.8615, 0.3807, 0.6209],\n",
      "        [0.8462, 0.6162, 0.8034],\n",
      "        [0.9611, 0.9449, 0.7675]], grad_fn=<AddmmBackward0>)\n",
      "train loss  0.020986659452319145\n",
      "outputs shape  tensor([[0.3000, 0.2476, 0.4141],\n",
      "        [0.8246, 0.6636, 0.8783],\n",
      "        [0.3988, 0.1904, 0.4425],\n",
      "        [0.9248, 0.9776, 0.8696],\n",
      "        [0.7332, 0.5900, 0.5212],\n",
      "        [0.8243, 0.6358, 0.8390],\n",
      "        [0.8442, 0.4946, 0.7640],\n",
      "        [0.7821, 0.3818, 0.6194],\n",
      "        [0.8503, 0.8899, 0.9369],\n",
      "        [0.4399, 0.2182, 0.5605],\n",
      "        [0.9073, 0.9222, 0.8359],\n",
      "        [0.7854, 0.5716, 0.8075],\n",
      "        [0.7821, 0.3538, 0.7739],\n",
      "        [0.2800, 0.1425, 0.2135],\n",
      "        [0.5054, 0.3261, 0.5669],\n",
      "        [0.9012, 0.5738, 0.9178]], grad_fn=<AddmmBackward0>)\n",
      "train loss  0.022936591878533363\n",
      "outputs shape  tensor([[0.8290, 0.4233, 0.7548],\n",
      "        [0.8127, 0.4675, 0.7100],\n",
      "        [0.5892, 0.3897, 0.4921],\n",
      "        [0.9768, 0.9119, 0.9178],\n",
      "        [0.8769, 0.3861, 0.7179],\n",
      "        [0.8246, 0.4168, 0.7523],\n",
      "        [0.8097, 0.5183, 0.7511],\n",
      "        [0.2571, 0.1186, 0.3370],\n",
      "        [0.8072, 0.5382, 0.9027],\n",
      "        [0.2600, 0.0863, 0.2626],\n",
      "        [0.9427, 0.9921, 0.8300],\n",
      "        [0.7533, 0.4909, 0.5653],\n",
      "        [0.8584, 0.6566, 0.9784],\n",
      "        [0.5503, 0.3183, 0.5134],\n",
      "        [0.9695, 0.5907, 0.9277],\n",
      "        [0.8486, 0.4194, 0.8120]], grad_fn=<AddmmBackward0>)\n",
      "train loss  0.019929593428969383\n",
      "outputs shape  tensor([[0.9173, 0.9237, 0.8503],\n",
      "        [0.5532, 0.3221, 0.5426],\n",
      "        [0.8832, 0.9114, 0.7599],\n",
      "        [0.8278, 0.4745, 0.6957],\n",
      "        [0.6947, 0.3746, 0.7133],\n",
      "        [0.8319, 0.9691, 0.8977],\n",
      "        [0.8782, 0.7076, 0.8221],\n",
      "        [0.8193, 0.4960, 0.7599],\n",
      "        [0.8494, 0.7190, 0.9317],\n",
      "        [0.4377, 0.3242, 0.4843],\n",
      "        [0.9670, 0.9496, 0.9796],\n",
      "        [0.7860, 0.4238, 0.7944],\n",
      "        [0.8755, 0.8806, 0.8684],\n",
      "        [0.5099, 0.3614, 0.6292],\n",
      "        [0.9928, 0.6004, 1.0043],\n",
      "        [0.8805, 0.5532, 0.8464]], grad_fn=<AddmmBackward0>)\n",
      "train loss  0.024875303730368614\n",
      "outputs shape  tensor([[0.3307, 0.2075, 0.4055],\n",
      "        [1.0147, 0.9855, 0.8917],\n",
      "        [0.8546, 0.5593, 0.8964],\n",
      "        [0.8229, 0.7239, 0.8938],\n",
      "        [0.8707, 0.5531, 0.5019],\n",
      "        [0.8482, 0.7740, 0.9185],\n",
      "        [0.9263, 1.1328, 0.9486],\n",
      "        [0.9925, 0.6727, 1.1278],\n",
      "        [0.8675, 0.9052, 0.9904],\n",
      "        [0.8893, 0.8403, 1.0792],\n",
      "        [0.8704, 0.5861, 0.9445],\n",
      "        [0.8833, 0.6435, 1.0234],\n",
      "        [0.8584, 0.7314, 0.9630],\n",
      "        [0.8998, 0.6543, 0.9601],\n",
      "        [0.9282, 0.8552, 0.9006],\n",
      "        [0.6144, 0.3784, 0.6049]], grad_fn=<AddmmBackward0>)\n",
      "train loss  0.020982688292860985\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[85]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m loss = loss_fn(outputs, batch[\u001b[33m'\u001b[39m\u001b[33mtargets\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mtrain loss \u001b[39m\u001b[33m\"\u001b[39m , loss.item())\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m optimizer.step()\n\u001b[32m     13\u001b[39m total_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kparv\\Downloads\\49595\\venv\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kparv\\Downloads\\49595\\venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kparv\\Downloads\\49595\\venv\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "for epoch in range(1):  # Adjust the number of epochs as needed\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch['input_ids'], batch['attention_mask'])\n",
    "        print(\"outputs shape \", outputs)\n",
    "        loss = loss_fn(outputs, batch['targets'])\n",
    "        print(\"train loss \" , loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            outputs = model(batch['input_ids'], batch['attention_mask'])\n",
    "            loss = loss_fn(outputs, batch['targets'])\n",
    "            print(\"validdation loss \", loss.item())\n",
    "            val_loss += loss.item()\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    # if (len(losses) > 1 and losses[-1] - avg_val_loss < 0.005):\n",
    "    #     print(\"Early stopping triggered.\")\n",
    "    #     break\n",
    "    losses.append((avg_val_loss))\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Training Loss: {avg_train_loss}, Validation Loss: {avg_val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "predictions = []\n",
    "actuals = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in DataLoader(test_dataset, batch_size=16):\n",
    "        outputs = model(batch['input_ids'], batch['attention_mask'])\n",
    "        # print(outputs)\n",
    "        # print(batch['targets'])\n",
    "        predictions.extend(outputs.numpy())\n",
    "        actuals.extend(batch['targets'].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formality - MSE: 0.021171515807509422, MAE: 0.1065855324268341, R2: 0.7062410116195679\n",
      "Tone - MSE: 0.018879776820540428, MAE: 0.10134880989789963, R2: 0.7875068783760071\n",
      "Conciseness - MSE: 0.019477585330605507, MAE: 0.11309678107500076, R2: 0.7169640064239502\n"
     ]
    }
   ],
   "source": [
    "\n",
    "predictions = np.array(predictions)\n",
    "actuals = np.array(actuals)\n",
    "\n",
    "# Calculate metrics for each output\n",
    "for i, label in enumerate(['Formality', 'Tone', 'Conciseness']):\n",
    "    mse = mean_squared_error(actuals[:, i], predictions[:, i])\n",
    "    mae = mean_absolute_error(actuals[:, i], predictions[:, i])\n",
    "    r2 = r2_score(actuals[:, i], predictions[:, i])\n",
    "    print(f\"{label} - MSE: {mse}, MAE: {mae}, R2: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "formality: 0.548\n",
      "tone: 0.397\n",
      "conciseness: 0.646\n"
     ]
    }
   ],
   "source": [
    "email_text = \"This is getting frustrating. Please send them ASAP.\"\n",
    "encoding = tokenizer.encode_plus(\n",
    "    email_text,\n",
    "    add_special_tokens=True,\n",
    "    max_length=128,\n",
    "    return_token_type_ids=False,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    return_attention_mask=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    input_ids = encoding[\"input_ids\"]\n",
    "    attention_mask = encoding[\"attention_mask\"]\n",
    "    output = model(input_ids, attention_mask)\n",
    "    prediction = output.numpy().flatten()\n",
    "\n",
    "\n",
    "labels = [\"formality\", \"tone\", \"conciseness\"]\n",
    "for label, score in zip(labels, prediction):\n",
    "    print(f\"{label}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('sentiment_model\\\\tokenizer_config.json',\n",
       " 'sentiment_model\\\\special_tokens_map.json',\n",
       " 'sentiment_model\\\\vocab.txt',\n",
       " 'sentiment_model\\\\added_tokens.json')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch, json, os\n",
    "\n",
    "MODEL_DIR = \"sentiment_model\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# --- after training finishes ---\n",
    "torch.save(model.state_dict(), f\"{MODEL_DIR}/model_weights.bin\")\n",
    "\n",
    "# optional but handy for auto‑reloading\n",
    "with open(f\"{MODEL_DIR}/model_config.json\", \"w\") as f:\n",
    "    json.dump(\n",
    "        {\n",
    "            \"base_model\": \"bert-base-uncased\",\n",
    "            \"num_outputs\": 3,\n",
    "            \"dropout\": 0.3\n",
    "        },\n",
    "        f,\n",
    "    )\n",
    "tokenizer.save_pretrained(MODEL_DIR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kparv\\Downloads\\49595\\Sentify\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision d7645e1 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sequence': 'After our last meeting, I wanted to follow up on the action items we discussed.', 'labels': ['follow-up', 'request', 'inform'], 'scores': [0.9452621340751648, 0.03445684537291527, 0.02028099074959755]}\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\n",
    "    \"zero-shot-classification\"  # or \"typeform/distilbert-base-uncased-mnli\"\n",
    ")\n",
    " \n",
    "# 2. Define your intent labels\n",
    "candidate_labels = [\"request\", \"inform\", \"follow-up\"]\n",
    " \n",
    "# 3. Classify a new email\n",
    "email_text = \"After our last meeting, I wanted to follow up on the action items we discussed.\"\n",
    "result = classifier(email_text, candidate_labels)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('intent_classification_dataset.csv')\n",
    "label_map = {'follow-up': 0, 'request': 1, 'inform': 2}\n",
    "df['label'] = df['label'].map(label_map)\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df['text'].tolist(), df['label'].tolist(), test_size=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True, return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class EmailDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.encodings['input_ids'][idx],\n",
    "            'attention_mask': self.encodings['attention_mask'][idx],\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = EmailDataset(train_encodings, train_labels)\n",
    "val_dataset = EmailDataset(val_encodings, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install accelerate>=0.26.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='180' max='180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [180/180 01:10, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.011900</td>\n",
       "      <td>0.007064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>0.002727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>0.002220</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=180, training_loss=0.11519228724969757, metrics={'train_runtime': 71.0246, 'train_samples_per_second': 20.275, 'train_steps_per_second': 2.534, 'total_flos': 5961139246080.0, 'train_loss': 0.11519228724969757, 'epoch': 3.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=3)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('follow-up', 0.9974984526634216)\n",
      "('request', 0.9977546334266663)\n",
      "('inform', 0.9976154565811157)\n"
     ]
    }
   ],
   "source": [
    "def classify_intent(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    outputs = model(**inputs)\n",
    "    probs = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "    pred = torch.argmax(probs, dim=1).item()\n",
    "    reverse_label_map = {0: 'follow-up', 1: 'request', 2: 'inform'}\n",
    "    return reverse_label_map[pred], probs[0][pred].item()\n",
    "\n",
    "\n",
    "output = classify_intent(\"I wanted to follow up on the action items we discussed.\")  \n",
    "print(output)  # ('follow-up', 0.85)\n",
    "\n",
    "output = classify_intent(\"Please send me the report.\")\n",
    "print(output)  # ('request', 0.92)\n",
    "\n",
    "output = classify_intent(\"The meeting is scheduled for next week.\")\n",
    "print(output)  # ('inform', 0.88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('intent_classification_model\\\\tokenizer_config.json',\n",
       " 'intent_classification_model\\\\special_tokens_map.json',\n",
       " 'intent_classification_model\\\\vocab.txt',\n",
       " 'intent_classification_model\\\\added_tokens.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"intent_classification_model\")\n",
    "tokenizer.save_pretrained(\"intent_classification_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pytorch_model.bin: 100%|██████████| 268M/268M [00:16<00:00, 16.0MB/s] \n",
      "c:\\Users\\kparv\\Downloads\\49595\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\kparv\\.cache\\huggingface\\hub\\models--parvk11--intent_classification_model. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/parvk11/intent_classification_model/commit/594e3a598a13b33f64b63b77032db22df953999a', commit_message='Upload tokenizer', commit_description='', oid='594e3a598a13b33f64b63b77032db22df953999a', pr_url=None, repo_url=RepoUrl('https://huggingface.co/parvk11/intent_classification_model', endpoint='https://huggingface.co', repo_type='model', repo_id='parvk11/intent_classification_model'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"intent_classification_model\")\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"intent_classification_model\")\n",
    "model.push_to_hub(\"intent_classification_model\", safe_serialization=False)\n",
    "tokenizer.push_to_hub(\"intent_classification_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
